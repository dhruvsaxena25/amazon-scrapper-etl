# Amazon Scraper Pipelines

Production-ready, Selenium-based pipelines for scraping Amazon search results and product details.

This package provides:

- A **URL scraping pipeline** to collect product URLs from Amazon search result pages.
- A **product scraping pipeline** to extract structured product data (price, specs, reviews, etc.) from those URLs.
- A **combined pipeline** that runs both stages endâ€‘toâ€‘end.

> **Note:** This project is proprietary. No one is allowed to use, copy, modify, or distribute any part of this code without explicit permission.

---

## Features

- **Modular design**: URL and product scraping pipelines can be run independently or together.
- **Configurable scraping**: Control search terms, number of URLs per term, timeouts, and headless mode.
- **Timestamped artifacts**: All outputs are stored under timestamped folders for easy versioning.
- **YAML-based locators**: Page locators are externalized into YAML for easier maintenance.
- **Detailed logging**: Structured logs for each stage and overall pipeline execution.
- **In-memory data access (optional)**: Optionally return scraped data as Python dicts in addition to JSON files.

---

## 1. URL Scraping Pipeline (`url_pipeline.py`)

Collects product URLs from Amazon search results and saves them to a JSON file.

### Basic Usage

```python
from scrapper.pipeline.url_pipeline import AmazonUrlScrapingPipeline

pipeline = AmazonUrlScrapingPipeline(
    search_terms=['laptop pc', 'wireless mouse'],
    target_links=[5, 3],  # 5 laptops, 3 mice
    headless=False
)

url_artifact = pipeline.run()
print(f"URLs saved to: {url_artifact.url_file_path}")
```

### Parameters

- `search_terms`: `list[str] | str`  
  One or more Amazon search terms (e.g. `"laptop pc"`).
- `target_links`: `int | list[int]` (default: `5`)  
  Number of URLs to scrape per search term.  
  - `int`: same count for all terms  
  - `list[int]`: per-term counts, must match `len(search_terms)`.
- `headless`: `bool` (default: `False`)  
  Run the browser in headless mode.
- `wait_timeout`: `int` (default: `5`)  
  Explicit wait timeout (seconds) for elements.
- `page_load_timeout`: `int` (default: `15`)  
  Page load timeout (seconds).
- `return_data`: `bool` (default: `False`)  
  If `True`, returns both the artifact and the inâ€‘memory URL data.

### Return Value

```python
# When return_data = False (default)
UrlDataArtifact(
    url_file_path="Artifacts/<timestamp>/UrlData/urls.json"
)

# When return_data = True
UrlDataArtifact, dict
```

### Example with in-memory URL data

```python
pipeline = AmazonUrlScrapingPipeline(
    search_terms=['laptop pc'],
    target_links=3,
    headless=True,
    return_data=True
)

url_artifact, url_data = pipeline.run()
print(url_data['total_urls'], "URLs collected")
```

### Standalone Execution

```bash
python url_pipeline.py
```

---

## 2. Product Scraping Pipeline (`product_pipeline.py`)

Reads a URL JSON file and scrapes detailed information for each product URL.

### Basic Usage

```python
from scrapper.pipeline.product_pipeline import AmazonProductScrapingPipeline

pipeline = AmazonProductScrapingPipeline(
    url_file_path="Artifacts/<timestamp>/UrlData/urls.json",
    headless=False
)

product_artifact = pipeline.run()
print(f"Products saved to: {product_artifact.product_file_path}")
print(f"Success: {product_artifact.scraped_count}")
```

### Parameters

- `url_file_path`: `str | Path` (required)  
  Path to the URL JSON generated by the URL pipeline.
- `headless`: `bool` (default: `False`)  
  Run the browser in headless mode.
- `wait_timeout`: `int` (default: `10`)  
  Explicit wait timeout (seconds) for elements.
- `page_load_timeout`: `int` (default: `20`)  
  Page load timeout (seconds).
- `return_prod_data`: `bool` (default: `False`)  
  If `True`, returns both the artifact and the inâ€‘memory product data.

### URL JSON Format (`urls.json`)

When running the product pipeline independently, the expected JSON format is:

```json
{
  "total_products": 2,
  "total_urls": 3,
  "products": {
    "search_term_1": {
      "count": 1,
      "urls": [
        "https://www.amazon.in/..."
      ]
    },
    "search_term_2": {
      "count": 2,
      "urls": [
        "https://www.amazon.in/...",
        "https://www.amazon.in/..."
      ]
    }
  }
}
```

### Return Value

```python
# When return_prod_data = False (default)
ProductDataArtifact(
    product_file_path="Artifacts/<timestamp>/ProductData/products.json",
    scraped_count=<int>,
    failed_count=<int>
)

# When return_prod_data = True
ProductDataArtifact, dict
```

### Example with in-memory product data

```python
pipeline = AmazonProductScrapingPipeline(
    url_file_path="Artifacts/<timestamp>/UrlData/urls.json",
    headless=True,
    return_prod_data=True
)

product_artifact, product_data = pipeline.run()
print(product_data['total_scraped'], "products scraped")
```

### Standalone Execution

```bash
# Ensure url_file_path in main() points to a valid urls.json
python product_pipeline.py
```

---

## 3. End-to-End Pipeline (`main_pipeline.py`)

Runs both URL and product scraping in sequence:  
**Search â†’ URLs â†’ Products**

### Basic Usage

```python
from scrapper.pipeline.main_pipeline import AmazonScrapingPipeline

pipeline = AmazonScrapingPipeline(
    search_terms=['wireless mouse'],
    target_links=1,
    headless=True,           # Set to False to watch the browser
    return_url_data=False,   # Optional: URL data in memory
    return_prod_data=True    # Optional: Product data in memory
)

result = pipeline.run_pipeline()

# Example: only product data returned in memory
url_artifact, product_artifact, product_data = result

print("\nâœ… Pipeline completed!")
print(f"ğŸ“ URLs file: {url_artifact.url_file_path}")
print(f"ğŸ“ Products file: {product_artifact.product_file_path}")
print(f"Scraped products: {product_data['total_scraped']}")
```

### Flags and Return Shapes

- `return_url_data=False`, `return_prod_data=False`  
  â†’ `(UrlDataArtifact, ProductDataArtifact)`
- `return_url_data=True`, `return_prod_data=False`  
  â†’ `(UrlDataArtifact, dict, ProductDataArtifact)`
- `return_url_data=False`, `return_prod_data=True`  
  â†’ `(UrlDataArtifact, ProductDataArtifact, dict)`
- `return_url_data=True`, `return_prod_data=True`  
  â†’ `(UrlDataArtifact, dict, ProductDataArtifact, dict)`

---

## Output Directory Structure

All artifacts are written under a timestamped directory:

```text
Artifacts/
â””â”€â”€ <timestamp>/                 # e.g. 12_04_2025_04_36_42
    â”œâ”€â”€ UrlData/
    â”‚   â””â”€â”€ urls.json            # Collected product URLs
    â””â”€â”€ ProductData/
        â””â”€â”€ products.json        # Detailed product data
```

---

## Project Layout

```text
project/
â”œâ”€â”€ Artifacts/
â”‚   â””â”€â”€ <timestamp_folder>/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ *.log
â”‚   â””â”€â”€ ...
â”œâ”€â”€ main.py
â””â”€â”€ scrapper/
    â”œâ”€â”€ config/
    â”‚   â”œâ”€â”€ urls_locators.yaml
    â”‚   â””â”€â”€ product_locators.yaml
    â”œâ”€â”€ constant/
    â”‚   â””â”€â”€ configuration.py
    â”œâ”€â”€ entity/
    â”‚   â”œâ”€â”€ artifact_entity.py
    â”‚   â”œâ”€â”€ config_entity.py
    â”‚   â”œâ”€â”€ product_locator_entity.py
    â”‚   â””â”€â”€ url_locator_entity.py
    â”œâ”€â”€ exception/
    â”‚   â””â”€â”€ custom_exception.py
    â”œâ”€â”€ logger/
    â”‚   â””â”€â”€ logging.py
    â”œâ”€â”€ pipeline/
    â”‚   â”œâ”€â”€ main_pipeline.py
    â”‚   â”œâ”€â”€ url_pipeline.py
    â”‚   â””â”€â”€ product_pipeline.py
    â”œâ”€â”€ router/
    â”‚   â””â”€â”€ api.py
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ multi_product_scrapper.py
    â”‚   â”œâ”€â”€ multi_url_scrapper.py
    â”‚   â””â”€â”€ url_scrapper.py
    â””â”€â”€ util/
        â””â”€â”€ main_utils.py
```

---

## Typical Workflows

### Option 1: Run End-to-End

```python
from scrapper.pipeline.main_pipeline import AmazonScrapingPipeline

pipeline = AmazonScrapingPipeline(
    search_terms=['laptop pc', 'wireless mouse'],
    target_links=[1, 2],
    headless=True,
    return_url_data=False,
    return_prod_data=True
)

url_artifact, product_artifact, product_data = pipeline.run_pipeline()
```

### Option 2: Run Stages Independently

```bash
# 1) Collect URLs
python url_pipeline.py

# 2) Update url_file_path in product_pipeline.py to the generated urls.json, then:
python product_pipeline.py
```

---

## Notes

- This code is tightly coupled to Amazon's current DOM structure and may need updates if Amazon changes its layout or anti-bot mechanisms.
- Use responsibly and in accordance with Amazon's terms of service and applicable laws.
- This project is proprietary. Do not use, copy, modify, or distribute without explicit permission.
